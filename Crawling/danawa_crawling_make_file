from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By

import time

import pandas as pd
import numpy as np

ser = Service('../chromedriver/chromedriver.exe')
options = webdriver.ChromeOptions()
options.add_experimental_option("excludeSwitches", ["enable-logging"])
driver = webdriver.Chrome(options=options)

url = 'https://search.danawa.com/dsearch.php?k1=%ED%95%B8%EB%93%9C%ED%8F%B0&module=goods&act=dispMain'
driver.get(url)

html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')

prod_items = soup.select('div.main_prodlist > ul.product_list > li.prod_item ')
len(prod_items)

def get_prod_items(pro_items):
    prod_data = []

    for prod_item in prod_items:
        try:
            title = prod_item.select('p.prod_name > a')[0].text
        except:
            title = ""

        try:
            spec_list = prod_item.select('div.spec_list')[0].text.strip()
        except:
            spec_list = ""

        try:
            price = prod_item.select('li.rank_one > p.price_sect > a > strong')[0].\
    text.strip().replace(',', "")
        except:
            price = 0

        mylist = [title, spec_list, price]

        prod_data.append(mylist)

    return(prod_data)

prod_data = get_prod_items(prod_items)
print(len(prod_data))

data = pd.DataFrame(prod_data)

data.columns = ['상품명', '스펙 목록', '가격']
data.to_excel('./files/danawa_crawling_result_class.xlsx', index =False)
